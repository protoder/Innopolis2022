Полный цикл подготовки файлоa

1) Формируем файлы для тренировки модели. Для этого мы
    а. К исходному набору данных прибавляем производные для всех точек
    б. Формируем дополнительные признаки с помощью библиотеки tsfresh
    ВНИМАНИЕ! Для выполнения этой задачи необходим GPU и CUDO.
    Colab блокнот с этой задачей хранится в файле "Store_Features.ipynb"

    Созданные файлы собираются в папке Input

2)  Выделяем из них оптимальный для нашей задачи набор признаков с помощью жадного алгоритма.
    Colab блокнот с этой задачей хранится в файле "BestFeatures.ipynb". Итог работы алгоритма -
    файл FeatureIndex3.npy списка оптимальных признаков. Его надо перенести в папку Inputs

3) Запуск GenerateInputs из входных данных, списка оптимальных признаков и файлов
   признаков формирует итоговые входные данные для обучения: файлы train_100.npy и valid_100.npy.
   Это 20-ти кратно аугментированные данные тестового и валидационного набора с добавленными признаками.
   Для валидации использованы первые 100 записей набора данных

4) Теперь надо с этими данными запустить генетический алгоритм для поиска лучших конфигураций. Сам генетический
   алгоритм ( базовый класс) реализован в модуле BaseGenetic.py. В Colab блокноте "xgboost_Features Genetic" - его
   реализация конкретно под xgboost и нашу задачу.

   Результатом работы алгоритма будет список файлов в корневой директории проекта формата XGB_5_*_0.9999.npy
   Это файлы так называемых хромосом  - они содержат данные о гиперпараметрах модели, и по ним однозначно можно ее
   воспроизвести.

5) Теперь полученные модели надо объединить в ансамбли. Выбрав из них наиболее подходящее сочетание. Для этого нужно
   воспользоваться модулем SelectBestAnsamble.py. В результате должен получиться файл Ansambles.ru
   состоящий из списка хромосом, к которым в конец добавлено одно число - рейтинг модели. Коэффициент, с которым будет
   учитываться ее голос

6) Все.



