# -*- coding: utf-8 -*-
"""Enopolis LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s16_kYImzqNrZ2ePWVmcxyKgfViFfbmy
"""

pip install sktime

from sklearn.neighbors import NearestNeighbors

import numpy as np
import random
from tensorflow.keras.models import Model, Sequential, load_model  # Импортируем модели keras: Model
from tensorflow.keras.layers import Input, Conv2DTranspose, concatenate, Dropout, Activation, Flatten, SpatialDropout2D, MaxPooling2D, \
    AveragePooling2D, Conv2D, Dense, LSTM, GRU, BatchNormalization, Reshape, Conv1D, MaxPooling1D  # Импортируем стандартные слои keras
from tensorflow.keras import backend as K  # Импортируем модуль backend keras'а
from tensorflow.keras.optimizers import Adam, RMSprop  # Импортируем оптимизатор Adam
from tensorflow.keras import \
    utils  # Импортируем модуль utils библиотеки tensorflow.keras для получения OHE-представления
from keras import regularizers
from keras.callbacks import Callback
import tensorflow as tf
import os
from tqdm.auto import tqdm
import glob

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from keras import backend as K
from tensorflow.keras.initializers import GlorotUniform, Orthogonal

import warnings
warnings.filterwarnings("ignore")

Colab = True
try:
    from google.colab import drive
except:
    Colab = False

if Colab:
    from google.colab import drive

    # Подключаем Google drive
    drive.mount('/content/drive')
    CrPath = "/content/drive/MyDrive/Uinnopolis/"

    import sys
    sys.path.append('/content/drive/MyDrive/Uinnopolis')
else:
    Acer = not os.path.exists("E:/Uinnopolis/")
    CrPath = "C:/Uinnopolis/" if Acer else "E:/Uinnopolis/"

from Libs import * #ReadCsv, WriteCsv, Graphic, Filter
from Experiments import *
from NN import *

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
tf.get_logger().setLevel('ERROR')

random.seed(1)

print('Reading Dataset')
X, Y = ReadCsv(CrPath, DelZeros = True, SortCtg = False, Train = True, Au = True)
XTest, YTest = ReadCsv(CrPath, DelZeros = True, SortCtg = True, Train = False, Au = True)
Data = (X, Y, XTest, YTest)
print(Y.shape)
X_train, X_test, y_train, y_test = train_test_split(X, Y[:, 1:2], test_size=0.03, random_state=42)
print(y_train.shape, y_test.shape)
X_train, y_train = TimeAugmentation(X_train, y_train, K=30, random=1, LevelK=0.2, Ver = 0, SinMode = False)
print(X_train.shape, y_train.shape)

def LSTMModel(XShape, N):
    random.seed(0)
    Inp = Input(shape=(XShape[-1], 1))

    XLSTM = LSTM(N, input_shape=(XShape[-1], 1), return_sequences=False,
                kernel_initializer=GlorotUniform(seed=1),
                recurrent_initializer=Orthogonal(seed=2))(Inp)
    #
    #XLSTM = BatchNormalization()(XLSTM)
    #XLSTM = GRU(100, return_sequences=True)(XLSTM)
    InpDense = Reshape((XShape[-1],))(Inp)
    InpDense = Dense(N, activation="relu", kernel_initializer=GlorotUniform(seed=3))(InpDense)
    XDENSE = concatenate([XLSTM, InpDense])

    XDENSE = Dense(800, activation="relu", kernel_initializer=GlorotUniform(seed=4))(XDENSE)
    XDENSE = Dropout(0.2)(XDENSE)

    XDENSE = Dense(50, activation="relu", kernel_initializer=GlorotUniform(seed=5))(XDENSE)
    XDENSE = Dropout(0.2)(XDENSE)
    XDENSE = Dense(7, activation="softmax", kernel_initializer=GlorotUniform(seed=6))(XDENSE)

    model = Model(inputs=Inp, outputs=XDENSE)

    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001), metrics=['accuracy'])
    #model.summary()
    print(N)
    return model

model_checkpoint_callback = ModelCheckpoint(
                filepath=f'{CrPath}GRU.h5',
                save_weights_only=False,
                monitor='val_accuracy',
                mode='min',
                save_best_only=True)


M = GRUModel(X_train.shape, 1000)
History = M.fit(X_train, to_categorical(y_train), validation_data = (X_test, to_categorical(y_test)), epochs = 1, 
          batch_size = 128, verbose = 1, callbacks = model_checkpoint_callback)

plt.figure(figsize=(14, 7))

if 'val_accuracy' in History.history:
    plt.plot(History.history['val_accuracy'],
          label='Доля верных ответов на тестовом наборе')
plt.plot(History.history['accuracy'],
          label='Доля верных ответов на тренировочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

for i in range(20):
    model_checkpoint_callback = ModelCheckpoint(
        filepath=f'{CrPath}LSTM{i+1}.h5',
        save_weights_only=False,
        monitor='val_accuracy',
        mode='max',
        verbose = 1,
        save_best_only=True)
    M = LSTMModel(X_train.shape, 600 + (i * 100))
    History = M.fit(X_train, to_categorical(y_train), validation_data = (X_test, to_categorical(y_test)), epochs = 50, 
              batch_size = 128, verbose = 1, callbacks = model_checkpoint_callback)
    
    WriteCsv(fr'{CrPath}LSTM{i+1}.csv', YTest[:,1], np.argmax(y_pred, -1))
    plt.figure(figsize=(14, 7))

    if 'val_accuracy' in History.history:
        plt.plot(History.history['val_accuracy'],
              label='Доля верных ответов на тестовом наборе')
    plt.plot(History.history['accuracy'],
              label='Доля верных ответов на тренировочном наборе')
    plt.xlabel('Эпоха обучения')
    plt.ylabel('Доля верных ответов')
    plt.legend()
    plt.show()

K.set_value(model.optimizer.learning_rate, 0.0001)

M = load_model(f'{CrPath}LSTM0.h5')
y_pred = M.predict(XTest)

WriteCsv(fr'{CrPath}LSTM0.csv', YTest[:,1], np.argmax(y_pred, -1))

from sktime.classification.kernel_based import RocketClassifier
from Libs import * #ReadCsv, WriteCsv, Graphic, Filter
from Experiments import *
from NN import *



X, Y = ReadCsv(CrPath, DelZeros = True, SortCtg = False, Train = True, RetPrc = True)
XTest, YTest = ReadCsv(CrPath, DelZeros = True, SortCtg = True, Train = False, Au = True)

X_train, X_test, y_train, y_test = train_test_split(X, Y[:, 1], test_size=0.1, random_state=42)

rocket = RocketClassifier()
rocket.fit(X_train, y_train)
y_pred = rocket.predict(X_test)
print(recall_score(y_test, y_pred, average="macro", zero_division=0))
print(0, classification_report(y_pred, y_test))

from sktime.classification.hybrid import HIVECOTEV2
hc2 = HIVECOTEV2(time_limit_in_minutes=1, random_state = 0)
hc2.fit(X_train, y_train)
y_pred = hc2.predict(X_test)
print(recall_score(y_test, y_pred, average="macro", zero_division=0))
print(0, classification_report(y_pred, y_test))
WriteCsv(fr'{CrPath}HV.csv', y_test, y_pred)

X_train, y_train = TimeAugmentation(X_train, y_train, K=10, random=1, LevelK=0.1, Ver = 0, SinMode = False)

from sklearn.ensemble import RandomForestClassifier

from sktime.pipeline import make_pipeline
from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor

tsfresh_trafo = TSFreshFeatureExtractor(default_fc_parameters="minimal")
randf = RandomForestClassifier(n_estimators=175)
pipe = make_pipeline(tsfresh_trafo, randf)

pipe.fit(X_train, y_train)
y_pred = pipe.predict (X_test)
accuracy_score (y_test, y_pred)

print(recall_score(y_test, y_pred, average="macro", zero_division=0))
print(0, classification_report(y_pred, y_test))
WriteCsv(fr'{CrPath}pipe.csv', y_test[:, 1:2], y_pred)

neigh = NearestNeighbors(n_neighbors=10, metric='cosine')
neigh.fit(X)

distances, idxs = neigh.kneighbors(queries_embeddings, 10, return_distance=True)

"""loss: 0.0515 - accuracy: 0.9840 - val_loss: 0.7139 - val_accuracy: 0.8390


"""





CrModel =  LSTMModel(LSTMLayers = [150], DenseLayers = [150], Epochs=30, Batch_size=128)
Filter = 0

SimpleExp = TExperiment(CrModel, Filter = Filter, InAugmentation = 1, OutAugmentation = 1, KAu = 10)
SimpleExp.Execute(Data, GenCsv = True, DoubleFit = False, SinMode = True)